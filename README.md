# OmniFoundry

> é–‹æºæ¨¡å‹æ¨è«–ç’°å¢ƒé›†æˆç¨‹å¼

ä¸€å€‹è‡ªå‹•åµæ¸¬ç¡¬é«”ã€æ™ºèƒ½é¸æ“‡æ¨è«–å¼•æ“çš„é–‹æº AI æ¨¡å‹ç®¡ç†èˆ‡éƒ¨ç½²å·¥å…·ã€‚

## ç‰¹è‰²åŠŸèƒ½

- ğŸ” **è‡ªå‹•ç¡¬é«”åµæ¸¬** - è‡ªå‹•è­˜åˆ¥ CPUã€GPUï¼ˆNVIDIA/AMD/Intelï¼‰å’Œè¨˜æ†¶é«”é…ç½®
- ğŸ¯ **æ™ºèƒ½å¼•æ“é¸æ“‡** - æ ¹æ“šç¡¬é«”å’Œæ¨¡å‹è‡ªå‹•é¸æ“‡æœ€ä½³æ¨è«–å¼•æ“
- ğŸ“¦ **æ¨¡å‹ç®¡ç†** - å¾ Hugging Face Hub æœå°‹ã€ä¸‹è¼‰å’Œç®¡ç†æ¨¡å‹
- ğŸš€ **å¤šå¼•æ“æ”¯æ´** - æ”¯æ´ Transformersã€llama.cppã€ONNX Runtimeã€Diffusers ç­‰
- ğŸŒ **API æœå‹™** - æä¾› OpenAI ç›¸å®¹çš„ REST API
- ğŸ–¥ï¸ **CLI å·¥å…·** - ç°¡å–®æ˜“ç”¨çš„å‘½ä»¤åˆ—ä»‹é¢
- ğŸ³ **Docker å®¹å™¨åŒ–** - é–‹ç®±å³ç”¨çš„å®¹å™¨æ˜ åƒ

## æ”¯æ´çš„æ¨¡å‹é¡å‹

- æ–‡å­—ç”Ÿæˆï¼ˆLLMï¼‰- GPTã€LLaMAã€Mistral ç­‰
- åœ–åƒç”Ÿæˆ - Stable Diffusionã€DALL-E ç­‰
- èªéŸ³è­˜åˆ¥/åˆæˆ - Whisperã€Bark ç­‰
- å¤šæ¨¡æ…‹ - CLIPã€LLaVA ç­‰

## å®‰è£

### å¾ PyPI å®‰è£ï¼ˆå³å°‡æ¨å‡ºï¼‰

```bash
pip install omnifoundry
```

### å¾åŸå§‹ç¢¼å®‰è£

```bash
git clone https://github.com/yourusername/omnifoundry.git
cd omnifoundry
pip install -e .
```

### å®‰è£é¡å¤–ä¾è³´

```bash
# å®‰è£ llama.cpp æ”¯æ´
pip install omnifoundry[llama-cpp]

# å®‰è£ ONNX Runtime æ”¯æ´
pip install omnifoundry[optimum]

# å®‰è£æ‰€æœ‰å¯é¸ä¾è³´
pip install omnifoundry[all]
```

## å¿«é€Ÿé–‹å§‹

### æª¢æŸ¥ç¡¬é«”è³‡è¨Š

```bash
omnifoundry info
```

### æœå°‹å’Œä¸‹è¼‰æ¨¡å‹

```bash
# æœå°‹æ¨¡å‹
omnifoundry list

# ä¸‹è¼‰æ¨¡å‹
omnifoundry download meta-llama/Llama-2-7b-hf
```

### åŸ·è¡Œæ¨è«–

```bash
omnifoundry run meta-llama/Llama-2-7b-hf --prompt "Hello, how are you?"
```

### å•Ÿå‹• API æœå‹™

```bash
omnifoundry serve meta-llama/Llama-2-7b-hf
```

ç„¶å¾Œå¯ä»¥ä½¿ç”¨ OpenAI ç›¸å®¹çš„ APIï¼š

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-hf",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Docker ä½¿ç”¨

```bash
# å»ºæ§‹æ˜ åƒ
docker build -t omnifoundry .

# åŸ·è¡Œå®¹å™¨ï¼ˆCPU ç‰ˆæœ¬ï¼‰
docker run -p 8000:8000 omnifoundry

# åŸ·è¡Œå®¹å™¨ï¼ˆGPU ç‰ˆæœ¬ï¼‰
docker run --gpus all -p 8000:8000 omnifoundry
```

æˆ–ä½¿ç”¨ docker-composeï¼š

```bash
docker-compose up
```

## é…ç½®

åœ¨ `configs/default.yaml` ä¸­è¨­å®šé è¨­é…ç½®ï¼š

```yaml
hardware:
  auto_detect: true
  force_cpu: false
  gpu_memory_fraction: 0.9

models:
  cache_dir: "./models"
  auto_download: true

inference:
  max_length: 2048
  temperature: 0.7
  
api:
  host: "0.0.0.0"
  port: 8000
```

## Python API ä½¿ç”¨

```python
from omnifoundry import OmniFoundry

# åˆå§‹åŒ–
foundry = OmniFoundry()

# è¼‰å…¥æ¨¡å‹
model = foundry.load_model("meta-llama/Llama-2-7b-hf")

# åŸ·è¡Œæ¨è«–
result = model.generate("Hello, how are you?")
print(result)
```

## å°ˆæ¡ˆæ¶æ§‹

```
OmniFoundry/
â”œâ”€â”€ omnifoundry/              # ä¸»è¦ Python å¥—ä»¶
â”‚   â”œâ”€â”€ core/                 # æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ hardware.py       # ç¡¬é«”è‡ªå‹•åµæ¸¬
â”‚   â”‚   â”œâ”€â”€ model_manager.py  # æ¨¡å‹ä¸‹è¼‰èˆ‡ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ inference.py      # æ¨è«–å¼•æ“æ•´åˆ
â”‚   â”‚   â””â”€â”€ config.py         # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ api/                  # FastAPI REST API
â”‚   â”œâ”€â”€ cli/                  # å‘½ä»¤åˆ—ä»‹é¢
â”‚   â””â”€â”€ utils/                # å·¥å…·å‡½æ•¸
â”œâ”€â”€ docker/                   # Docker ç›¸é—œ
â”œâ”€â”€ configs/                  # é…ç½®ç¯„ä¾‹
â”œâ”€â”€ examples/                 # ä½¿ç”¨ç¯„ä¾‹
â””â”€â”€ tests/                    # æ¸¬è©¦
```

## é–‹ç™¼

### å®‰è£é–‹ç™¼ä¾è³´

```bash
pip install -e .[dev]
```

### åŸ·è¡Œæ¸¬è©¦

```bash
pytest tests/
```

### ç¨‹å¼ç¢¼æ ¼å¼åŒ–

```bash
black omnifoundry/
flake8 omnifoundry/
```

## è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

## æˆæ¬Š

MIT License

## è¯çµ¡æ–¹å¼

- GitHub: https://github.com/yourusername/omnifoundry
- Issues: https://github.com/yourusername/omnifoundry/issues

## è‡´è¬

æ„Ÿè¬ä»¥ä¸‹å°ˆæ¡ˆæä¾›çš„éˆæ„Ÿå’ŒæŠ€è¡“æ”¯æ´ï¼š
- Hugging Face Transformers
- llama.cpp
- ONNX Runtime
- FastAPI

