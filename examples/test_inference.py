"""
æ¸¬è©¦ Transformers æ¨è«–å¼•æ“

ä½¿ç”¨è¼•é‡ç´šæ¨¡å‹æ¸¬è©¦æ¨è«–åŠŸèƒ½
"""

import logging
from omnifoundry.core.inference import TransformersEngine

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_basic_inference():
    """æ¸¬è©¦åŸºæœ¬æ¨è«–åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦ 1: åŸºæœ¬æ–‡å­—ç”Ÿæˆ")
    print("="*60)
    
    # ä½¿ç”¨ GPT-2ï¼ˆè¼•é‡ç´šï¼Œå¿«é€Ÿä¸‹è¼‰ï¼‰
    model_id = "gpt2"
    print(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    
    # å»ºç«‹å¼•æ“ï¼ˆä½¿ç”¨ CPU æ¨¡å¼ä»¥ç¢ºä¿ç›¸å®¹æ€§ï¼‰
    engine = TransformersEngine(model_id, device="cpu", dtype="float32", quantization=None)
    
    # è¼‰å…¥æ¨¡å‹
    print("\næ­£åœ¨è¼‰å…¥æ¨¡å‹...")
    engine.load_model()
    
    # åŸ·è¡Œæ¨è«–
    prompt = "Once upon a time"
    print(f"\næç¤ºè©: {prompt}")
    print("="*60)
    
    result = engine.infer(
        prompt,
        max_new_tokens=50,
        temperature=0.8,
        do_sample=True,
    )
    
    print(f"ç”Ÿæˆçµæœ:\n{result}")
    print("="*60)
    
    # å¸è¼‰æ¨¡å‹
    engine.unload_model()
    print("âœ… æ¸¬è©¦ 1 å®Œæˆ\n")


def test_stream_inference():
    """æ¸¬è©¦ä¸²æµæ¨è«–"""
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦ 2: ä¸²æµæ–‡å­—ç”Ÿæˆ")
    print("="*60)
    
    model_id = "gpt2"
    print(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    
    engine = TransformersEngine(model_id, device="cpu", dtype="float32", quantization=None)
    engine.load_model()
    
    prompt = "The future of artificial intelligence is"
    print(f"\næç¤ºè©: {prompt}")
    print("="*60)
    print("ä¸²æµè¼¸å‡º: ", end="", flush=True)
    
    for token in engine.stream(prompt, max_new_tokens=30, temperature=0.8):
        print(token, end="", flush=True)
    
    print("\n" + "="*60)
    
    engine.unload_model()
    print("âœ… æ¸¬è©¦ 2 å®Œæˆ\n")


def test_batch_inference():
    """æ¸¬è©¦æ‰¹æ¬¡æ¨è«–"""
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦ 3: æ‰¹æ¬¡æ¨è«–")
    print("="*60)
    
    model_id = "gpt2"
    print(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    
    engine = TransformersEngine(model_id, device="cpu", dtype="float32", quantization=None)
    engine.load_model()
    
    prompts = [
        "Artificial intelligence is",
        "The best programming language is",
        "In the year 2050,",
    ]
    
    print(f"\næ‰¹æ¬¡è™•ç† {len(prompts)} å€‹æç¤ºè©...")
    print("="*60)
    
    results = engine.infer(
        prompts,
        max_new_tokens=20,
        temperature=0.7,
    )
    
    for i, (prompt, result) in enumerate(zip(prompts, results), 1):
        print(f"\næç¤º {i}: {prompt}")
        print(f"çµæœ: {result}")
    
    print("="*60)
    
    engine.unload_model()
    print("âœ… æ¸¬è©¦ 3 å®Œæˆ\n")


def test_chinese_model():
    """æ¸¬è©¦ä¸­æ–‡æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦ 4: ä¸­æ–‡æ¨¡å‹æ¨è«–")
    print("="*60)
    
    # ä½¿ç”¨ Qwen2.5-0.5Bï¼ˆè¼•é‡ç´šä¸­æ–‡æ¨¡å‹ï¼‰
    model_id = "Qwen/Qwen2.5-0.5B-Instruct"
    print(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    print("æ³¨æ„: æ­¤æ¨¡å‹ç´„ 1GBï¼Œé¦–æ¬¡ä¸‹è¼‰å¯èƒ½éœ€è¦æ™‚é–“")
    
    try:
        # å»ºç«‹å¼•æ“ï¼Œæ‰‹å‹•è¨­å®šä¸ä½¿ç”¨é‡åŒ–ï¼ˆå°æ¨¡å‹ï¼‰
        engine = TransformersEngine(
            model_id,
            trust_remote_code=True,
            quantization=None,  # ä¸ä½¿ç”¨é‡åŒ–
        )
        
        engine.load_model()
        
        # æ¸¬è©¦å°è©±åŠŸèƒ½
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æœ‰å¹«åŠ©çš„ AI åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": "è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹äººå·¥æ™ºæ…§ã€‚"},
        ]
        
        print("\nå°è©±æ¸¬è©¦:")
        print("="*60)
        print(f"User: {messages[1]['content']}")
        
        response = engine.chat(messages, max_new_tokens=100, temperature=0.7)
        print(f"Assistant: {response}")
        print("="*60)
        
        engine.unload_model()
        print("âœ… æ¸¬è©¦ 4 å®Œæˆ\n")
        
    except Exception as e:
        print(f"âš ï¸  æ¸¬è©¦ 4 è·³é: {e}")
        print("å¯èƒ½æ˜¯å› ç‚ºæ¨¡å‹ä¸‹è¼‰å¤±æ•—æˆ–è¨˜æ†¶é«”ä¸è¶³\n")


def test_custom_config():
    """æ¸¬è©¦è‡ªè¨‚é…ç½®"""
    print("\n" + "="*60)
    print("ğŸ§ª æ¸¬è©¦ 5: è‡ªè¨‚é…ç½®")
    print("="*60)
    
    model_id = "gpt2"
    print(f"ä½¿ç”¨æ¨¡å‹: {model_id}")
    
    # æ‰‹å‹•è¨­å®šé…ç½®ï¼ˆä¸è‡ªå‹•åµæ¸¬ï¼‰
    engine = TransformersEngine(
        model_id,
        auto_config=False,  # ä¸è‡ªå‹•é…ç½®
        device="cpu",       # å¼·åˆ¶ä½¿ç”¨ CPU
        dtype="float32",    # ä½¿ç”¨ float32
        quantization=None,  # ä¸ä½¿ç”¨é‡åŒ–
    )
    
    print(f"\né…ç½®: device={engine.config['device']}, "
          f"dtype={engine.config['dtype']}, "
          f"quantization={engine.config['quantization']}")
    
    engine.load_model()
    
    result = engine.infer(
        "Hello, I am a",
        max_new_tokens=10,
        temperature=0.5,
    )
    
    print(f"\nç”Ÿæˆçµæœ: {result}")
    print("="*60)
    
    engine.unload_model()
    print("âœ… æ¸¬è©¦ 5 å®Œæˆ\n")


def main():
    """ä¸»å‡½æ•¸"""
    print("\n" + "ğŸš€ "* 20)
    print("OmniFoundry Transformers å¼•æ“æ¸¬è©¦")
    print("ğŸš€ "* 20)
    
    try:
        # åŸ·è¡Œæ¸¬è©¦
        test_basic_inference()
        test_stream_inference()
        test_batch_inference()
        test_custom_config()
        
        # ä¸­æ–‡æ¨¡å‹æ¸¬è©¦ï¼ˆå¯é¸ï¼‰
        user_input = input("\næ˜¯å¦æ¸¬è©¦ä¸­æ–‡æ¨¡å‹ï¼Ÿ(y/nï¼Œéœ€è¦ä¸‹è¼‰ç´„ 1GB): ")
        if user_input.lower() == 'y':
            test_chinese_model()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
        print("="*60 + "\n")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

