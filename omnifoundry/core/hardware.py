"""
ç¡¬é«”è‡ªå‹•åµæ¸¬æ¨¡çµ„

è² è²¬åµæ¸¬ç³»çµ±ç¡¬é«”é…ç½®ï¼ŒåŒ…æ‹¬ï¼š
- CPU é¡å‹ã€æ ¸å¿ƒæ•¸ã€è¨˜æ†¶é«”å¤§å°
- NVIDIA/AMD/Intel GPU åŠ VRAM
- CUDA/ROCm/OneAPI å¯ç”¨æ€§
- æ ¹æ“šç¡¬é«”è‡ªå‹•æ¨è–¦é…ç½®
"""

import platform
import psutil
from typing import Dict, List, Optional, Any
import logging

logger = logging.getLogger(__name__)


class HardwareDetector:
    """ç¡¬é«”åµæ¸¬å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¡¬é«”åµæ¸¬å™¨"""
        self._cpu_info = None
        self._gpu_info = None
        self._memory_info = None
        
    def detect_cpu(self) -> Dict[str, Any]:
        """
        åµæ¸¬ CPU è³‡è¨Š
        
        Returns:
            åŒ…å« CPU è³‡è¨Šçš„å­—å…¸
        """
        if self._cpu_info is not None:
            return self._cpu_info
            
        try:
            # åŸºæœ¬ CPU è³‡è¨Š
            cpu_info = {
                "architecture": platform.machine(),
                "processor": platform.processor(),
                "physical_cores": psutil.cpu_count(logical=False),
                "logical_cores": psutil.cpu_count(logical=True),
                "max_frequency": None,
                "min_frequency": None,
                "current_frequency": None,
            }
            
            # å–å¾— CPU é »ç‡è³‡è¨Š
            try:
                cpu_freq = psutil.cpu_freq()
                if cpu_freq:
                    cpu_info["max_frequency"] = f"{cpu_freq.max:.2f} MHz"
                    cpu_info["min_frequency"] = f"{cpu_freq.min:.2f} MHz"
                    cpu_info["current_frequency"] = f"{cpu_freq.current:.2f} MHz"
            except Exception as e:
                logger.warning(f"ç„¡æ³•ç²å– CPU é »ç‡è³‡è¨Š: {e}")
            
            # å˜—è©¦ä½¿ç”¨ cpuinfo ç²å–æ›´è©³ç´°è³‡è¨Š
            try:
                import cpuinfo
                cpu_data = cpuinfo.get_cpu_info()
                cpu_info["brand"] = cpu_data.get("brand_raw", "Unknown")
                cpu_info["vendor"] = cpu_data.get("vendor_id_raw", "Unknown")
                cpu_info["flags"] = cpu_data.get("flags", [])
            except ImportError:
                logger.info("py-cpuinfo æœªå®‰è£ï¼Œä½¿ç”¨åŸºæœ¬ CPU è³‡è¨Š")
                cpu_info["brand"] = platform.processor()
                cpu_info["vendor"] = "Unknown"
                cpu_info["flags"] = []
            except Exception as e:
                logger.warning(f"ç²å–è©³ç´° CPU è³‡è¨Šæ™‚å‡ºéŒ¯: {e}")
                cpu_info["brand"] = platform.processor()
                cpu_info["vendor"] = "Unknown"
                cpu_info["flags"] = []
            
            self._cpu_info = cpu_info
            return cpu_info
            
        except Exception as e:
            logger.error(f"åµæ¸¬ CPU æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "error": str(e),
                "physical_cores": 0,
                "logical_cores": 0,
            }
    
    def detect_gpu(self) -> List[Dict[str, Any]]:
        """
        åµæ¸¬ GPU è³‡è¨Š
        
        Returns:
            GPU è³‡è¨Šåˆ—è¡¨
        """
        if self._gpu_info is not None:
            return self._gpu_info
            
        gpus = []
        
        # åµæ¸¬ NVIDIA GPU (CUDA)
        nvidia_gpus = self._detect_nvidia_gpu()
        gpus.extend(nvidia_gpus)
        
        # åµæ¸¬ AMD GPU (ROCm)
        amd_gpus = self._detect_amd_gpu()
        gpus.extend(amd_gpus)
        
        # åµæ¸¬ Intel GPU (OneAPI)
        intel_gpus = self._detect_intel_gpu()
        gpus.extend(intel_gpus)
        
        # å¦‚æœæ²’æœ‰åµæ¸¬åˆ°ä»»ä½• GPU
        if not gpus:
            logger.info("æœªåµæ¸¬åˆ°ç¨ç«‹ GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼")
            gpus.append({
                "id": -1,
                "name": "CPU (ç„¡ GPU)",
                "vendor": "CPU",
                "memory_total": 0,
                "memory_free": 0,
                "compute_capability": None,
                "available": True,
            })
        
        self._gpu_info = gpus
        return gpus
    
    def _detect_nvidia_gpu(self) -> List[Dict[str, Any]]:
        """åµæ¸¬ NVIDIA GPU"""
        gpus = []
        
        # æ–¹æ³• 1: ä½¿ç”¨ PyTorch
        try:
            import torch
            if torch.cuda.is_available():
                cuda_available = True
                device_count = torch.cuda.device_count()
                
                for i in range(device_count):
                    props = torch.cuda.get_device_properties(i)
                    memory_total = props.total_memory / (1024**3)  # è½‰æ›ç‚º GB
                    
                    gpu_info = {
                        "id": i,
                        "name": props.name,
                        "vendor": "NVIDIA",
                        "memory_total": f"{memory_total:.2f} GB",
                        "memory_free": None,  # ç¨å¾Œæ›´æ–°
                        "compute_capability": f"{props.major}.{props.minor}",
                        "cuda_available": True,
                        "available": True,
                    }
                    
                    # å˜—è©¦ç²å–ç•¶å‰å¯ç”¨è¨˜æ†¶é«”
                    try:
                        memory_free = torch.cuda.mem_get_info(i)[0] / (1024**3)
                        gpu_info["memory_free"] = f"{memory_free:.2f} GB"
                    except:
                        pass
                    
                    gpus.append(gpu_info)
                    
                logger.info(f"é€é PyTorch åµæ¸¬åˆ° {device_count} å€‹ NVIDIA GPU")
                return gpus
        except ImportError:
            logger.debug("PyTorch æœªå®‰è£ï¼Œç„¡æ³•é€é PyTorch åµæ¸¬ NVIDIA GPU")
        except Exception as e:
            logger.debug(f"é€é PyTorch åµæ¸¬ NVIDIA GPU æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # æ–¹æ³• 2: ä½¿ç”¨ GPUtil
        try:
            import GPUtil
            gpu_list = GPUtil.getGPUs()
            
            for i, gpu in enumerate(gpu_list):
                gpu_info = {
                    "id": gpu.id,
                    "name": gpu.name,
                    "vendor": "NVIDIA",
                    "memory_total": f"{gpu.memoryTotal / 1024:.2f} GB",
                    "memory_free": f"{gpu.memoryFree / 1024:.2f} GB",
                    "memory_used": f"{gpu.memoryUsed / 1024:.2f} GB",
                    "gpu_load": f"{gpu.load * 100:.1f}%",
                    "temperature": f"{gpu.temperature}Â°C",
                    "compute_capability": None,
                    "cuda_available": True,
                    "available": True,
                }
                gpus.append(gpu_info)
            
            if gpus:
                logger.info(f"é€é GPUtil åµæ¸¬åˆ° {len(gpus)} å€‹ NVIDIA GPU")
            return gpus
            
        except ImportError:
            logger.debug("GPUtil æœªå®‰è£ï¼Œç„¡æ³•é€é GPUtil åµæ¸¬ NVIDIA GPU")
        except Exception as e:
            logger.debug(f"é€é GPUtil åµæ¸¬ NVIDIA GPU æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return gpus
    
    def _detect_amd_gpu(self) -> List[Dict[str, Any]]:
        """åµæ¸¬ AMD GPU (ROCm)"""
        gpus = []
        
        try:
            import torch
            if hasattr(torch, 'hip') and torch.hip.is_available():
                device_count = torch.hip.device_count()
                
                for i in range(device_count):
                    gpu_info = {
                        "id": i,
                        "name": torch.hip.get_device_name(i),
                        "vendor": "AMD",
                        "memory_total": None,
                        "memory_free": None,
                        "rocm_available": True,
                        "available": True,
                    }
                    gpus.append(gpu_info)
                
                logger.info(f"åµæ¸¬åˆ° {device_count} å€‹ AMD GPU (ROCm)")
        except Exception as e:
            logger.debug(f"åµæ¸¬ AMD GPU æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return gpus
    
    def _detect_intel_gpu(self) -> List[Dict[str, Any]]:
        """åµæ¸¬ Intel GPU (OneAPI)"""
        gpus = []
        
        try:
            import torch
            if hasattr(torch, 'xpu') and torch.xpu.is_available():
                device_count = torch.xpu.device_count()
                
                for i in range(device_count):
                    gpu_info = {
                        "id": i,
                        "name": f"Intel GPU {i}",
                        "vendor": "Intel",
                        "memory_total": None,
                        "memory_free": None,
                        "oneapi_available": True,
                        "available": True,
                    }
                    gpus.append(gpu_info)
                
                logger.info(f"åµæ¸¬åˆ° {device_count} å€‹ Intel GPU (OneAPI)")
        except Exception as e:
            logger.debug(f"åµæ¸¬ Intel GPU æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return gpus
    
    def detect_memory(self) -> Dict[str, Any]:
        """
        åµæ¸¬è¨˜æ†¶é«”è³‡è¨Š
        
        Returns:
            åŒ…å«è¨˜æ†¶é«”è³‡è¨Šçš„å­—å…¸
        """
        if self._memory_info is not None:
            return self._memory_info
            
        try:
            mem = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            memory_info = {
                "total": f"{mem.total / (1024**3):.2f} GB",
                "available": f"{mem.available / (1024**3):.2f} GB",
                "used": f"{mem.used / (1024**3):.2f} GB",
                "percent_used": f"{mem.percent}%",
                "swap_total": f"{swap.total / (1024**3):.2f} GB",
                "swap_used": f"{swap.used / (1024**3):.2f} GB",
                "swap_percent": f"{swap.percent}%",
                # åŸå§‹å€¼ï¼ˆç”¨æ–¼è¨ˆç®—ï¼‰
                "_total_bytes": mem.total,
                "_available_bytes": mem.available,
            }
            
            self._memory_info = memory_info
            return memory_info
            
        except Exception as e:
            logger.error(f"åµæ¸¬è¨˜æ†¶é«”æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                "error": str(e),
                "total": "0 GB",
                "available": "0 GB",
            }
    
    def get_hardware_info(self) -> Dict[str, Any]:
        """
        ç²å–å®Œæ•´ç¡¬é«”è³‡è¨Š
        
        Returns:
            åŒ…å«æ‰€æœ‰ç¡¬é«”è³‡è¨Šçš„å­—å…¸
        """
        return {
            "system": {
                "os": platform.system(),
                "os_version": platform.version(),
                "platform": platform.platform(),
                "python_version": platform.python_version(),
            },
            "cpu": self.detect_cpu(),
            "gpu": self.detect_gpu(),
            "memory": self.detect_memory(),
        }
    
    def recommend_config(self, model_type: str = "llm", model_size: str = "7b") -> Dict[str, Any]:
        """
        æ ¹æ“šç¡¬é«”æ¨è–¦é…ç½®
        
        Args:
            model_type: æ¨¡å‹é¡å‹ (llm, diffusion, etc.)
            model_size: æ¨¡å‹å¤§å° (7b, 13b, 70b, etc.)
            
        Returns:
            æ¨è–¦çš„é…ç½®å­—å…¸
        """
        cpu_info = self.detect_cpu()
        gpu_info = self.detect_gpu()
        memory_info = self.detect_memory()
        
        # è§£æè¨˜æ†¶é«”å¤§å°ï¼ˆGBï¼‰
        total_ram_gb = memory_info.get("_total_bytes", 0) / (1024**3)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
        has_gpu = any(gpu.get("vendor") != "CPU" for gpu in gpu_info)
        
        # åŸºç¤é…ç½®
        config = {
            "device": "cpu",
            "dtype": "float32",
            "quantization": None,
            "batch_size": 1,
            "max_length": 2048,
            "use_flash_attention": False,
            "cpu_offload": False,
        }
        
        # æ ¹æ“š GPU æƒ…æ³èª¿æ•´
        if has_gpu:
            nvidia_gpus = [g for g in gpu_info if g.get("vendor") == "NVIDIA"]
            
            if nvidia_gpus:
                # ä½¿ç”¨ç¬¬ä¸€å€‹ NVIDIA GPU
                gpu = nvidia_gpus[0]
                config["device"] = "cuda"
                
                # è§£æ VRAM å¤§å°
                vram_str = gpu.get("memory_total", "0 GB")
                try:
                    vram_gb = float(vram_str.split()[0])
                except:
                    vram_gb = 0
                
                # æ ¹æ“š VRAM å’Œæ¨¡å‹å¤§å°æ¨è–¦é…ç½®
                if model_type == "llm":
                    if "70b" in model_size.lower() or "65b" in model_size.lower():
                        # å¤§å‹æ¨¡å‹ (70B)
                        if vram_gb >= 80:
                            config["dtype"] = "float16"
                            config["quantization"] = None
                            config["batch_size"] = 4
                        elif vram_gb >= 48:
                            config["dtype"] = "float16"
                            config["quantization"] = "int8"
                            config["batch_size"] = 2
                        else:
                            config["dtype"] = "float16"
                            config["quantization"] = "int4"
                            config["batch_size"] = 1
                            config["cpu_offload"] = True
                    
                    elif "13b" in model_size.lower():
                        # ä¸­å‹æ¨¡å‹ (13B)
                        if vram_gb >= 24:
                            config["dtype"] = "float16"
                            config["quantization"] = None
                            config["batch_size"] = 4
                        elif vram_gb >= 12:
                            config["dtype"] = "float16"
                            config["quantization"] = "int8"
                            config["batch_size"] = 2
                        else:
                            config["dtype"] = "float16"
                            config["quantization"] = "int4"
                            config["batch_size"] = 1
                    
                    else:
                        # å°å‹æ¨¡å‹ (7B æˆ–æ›´å°)
                        if vram_gb >= 16:
                            config["dtype"] = "float16"
                            config["quantization"] = None
                            config["batch_size"] = 8
                            config["use_flash_attention"] = True
                        elif vram_gb >= 8:
                            config["dtype"] = "float16"
                            config["quantization"] = "int8"
                            config["batch_size"] = 4
                        else:
                            config["dtype"] = "float16"
                            config["quantization"] = "int4"
                            config["batch_size"] = 2
                
                elif model_type == "diffusion":
                    # åœ–åƒç”Ÿæˆæ¨¡å‹
                    if vram_gb >= 12:
                        config["dtype"] = "float16"
                        config["batch_size"] = 4
                    elif vram_gb >= 8:
                        config["dtype"] = "float16"
                        config["batch_size"] = 2
                    else:
                        config["dtype"] = "float16"
                        config["batch_size"] = 1
        
        # ç´” CPU æ¨¡å¼é…ç½®
        else:
            config["device"] = "cpu"
            
            if total_ram_gb >= 32:
                config["dtype"] = "float32"
                config["batch_size"] = 2
            elif total_ram_gb >= 16:
                config["dtype"] = "float32"
                config["batch_size"] = 1
                config["quantization"] = "int8"
            else:
                config["dtype"] = "float32"
                config["batch_size"] = 1
                config["quantization"] = "int4"
            
            # CPU å»ºè­°ä½¿ç”¨é‡åŒ–æ¨¡å‹
            if not config["quantization"]:
                config["quantization"] = "int8"
        
        return config
    
    def print_hardware_info(self):
        """ä»¥å‹å–„æ ¼å¼å°å‡ºç¡¬é«”è³‡è¨Š"""
        info = self.get_hardware_info()
        
        print("\n" + "="*60)
        print("ğŸ–¥ï¸  ç³»çµ±ç¡¬é«”è³‡è¨Š")
        print("="*60)
        
        # ç³»çµ±è³‡è¨Š
        print("\nã€ç³»çµ±ã€‘")
        print(f"  ä½œæ¥­ç³»çµ±: {info['system']['os']} {info['system']['platform']}")
        print(f"  Python ç‰ˆæœ¬: {info['system']['python_version']}")
        
        # CPU è³‡è¨Š
        print("\nã€CPUã€‘")
        cpu = info['cpu']
        print(f"  è™•ç†å™¨: {cpu.get('brand', 'Unknown')}")
        print(f"  æ¶æ§‹: {cpu.get('architecture', 'Unknown')}")
        print(f"  å¯¦é«”æ ¸å¿ƒ: {cpu.get('physical_cores', 0)}")
        print(f"  é‚è¼¯æ ¸å¿ƒ: {cpu.get('logical_cores', 0)}")
        if cpu.get('current_frequency'):
            print(f"  ç•¶å‰é »ç‡: {cpu['current_frequency']}")
        
        # GPU è³‡è¨Š
        print("\nã€GPUã€‘")
        gpus = info['gpu']
        if gpus:
            for i, gpu in enumerate(gpus):
                if gpu.get('vendor') == 'CPU':
                    print(f"  æœªåµæ¸¬åˆ°ç¨ç«‹ GPU")
                else:
                    print(f"  GPU {i}: {gpu.get('name', 'Unknown')}")
                    print(f"    å» å•†: {gpu.get('vendor', 'Unknown')}")
                    if gpu.get('memory_total'):
                        print(f"    é¡¯å­˜: {gpu['memory_total']}", end="")
                        if gpu.get('memory_free'):
                            print(f" (å¯ç”¨: {gpu['memory_free']})", end="")
                        print()
                    if gpu.get('compute_capability'):
                        print(f"    è¨ˆç®—èƒ½åŠ›: {gpu['compute_capability']}")
                    if gpu.get('gpu_load'):
                        print(f"    è² è¼‰: {gpu['gpu_load']}")
        else:
            print("  æœªåµæ¸¬åˆ° GPU")
        
        # è¨˜æ†¶é«”è³‡è¨Š
        print("\nã€è¨˜æ†¶é«”ã€‘")
        mem = info['memory']
        print(f"  ç¸½å®¹é‡: {mem.get('total', 'Unknown')}")
        print(f"  å¯ç”¨: {mem.get('available', 'Unknown')}")
        print(f"  å·²ç”¨: {mem.get('used', 'Unknown')} ({mem.get('percent_used', '0%')})")
        
        print("\n" + "="*60 + "\n")

